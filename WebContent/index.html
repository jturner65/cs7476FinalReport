<html>
<head>
<title>Advance Computer Vision Final Project For John Turner and Siddharth Raja : Face-Audio-Text dataset and LSTM-based lipreader</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 16px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 20px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 960px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

td img {
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>John Turner and Siddharth Raja </h1>
</div>
</div>
<div class="container">

<h2>CS 7476 Final Project : O'FaMACap dataset (Obama Face&Mouth Image/Audio/Caption) and LSTM-based lipreader.</h2>
<div style="float: right; padding: 20px">
<img src="images/out_011_xvid_2265.jpg" width=320 height=180/>
<p style="font-size: 14px">Figure 1. Screen Capture from a presidential address.</p>
</div>
<h3>Abstract</h3>
<p>We present a dataset of aligned images, audio samples, and captions built from the President Obama's weekly addresses 
over the time period of 2009 to 2015, yielding a corpus of over 1.5 million samples and 205,000 conversationally spoken words.
We also present the tools and methods we used to collect the data, with which a user can extract the same data from 
videos with provided closed caption files.  Two pretrained 10,000 node Self-Organizing Maps are also included, one trained on half of the audio data samples and one
trained on the mouth image samples, to be used to faciliate working with the high dimensional data.  Lastly, we present an
LSTM-based network trained on our dataset to associate sequences of mouth motions to captions (lipreading) to demonstrate the set's utility.
</p>
<h3>1.Introduction</h3>
<p>There are many interesting problems in Computer Vision that depend upon more than a single image for a solution.  Beyond the inherent complexities of 
recognition and classification commonly addressed in the field, with some tasks a single image doesn't present enough information, and instead sequences of images 
are necessary to even attempt to decipher the desired result.</p>
<p>One such problem is lipreading, the recognition of spoken words through interpreting the motions of the speaker's face.  This is difficult to accomplish using a purely
instance-based classification mechanism, since, taking the english language for example, there are fewer unique mouth shapes a speaker will produce while speaking
than there are unique units of sounds (called phonemes) that that speaker will be able to generate, rendering a one-to-one mapping between the two impossible.</p>
<p>These kinds of problems add another dimension, literally, to the already high dimensional problem space inherent in Computer Vision tasks, and require specialized 
algorithms that can learn sequential data, such as recurrent neural nets, to achieve useful results, along with specialized datasets by which these networks need to be trained.  This
is a challenge we hope to help address with our dataset.</p>
<p>We also present tools where similar data can be collected easily, as well as algorithms that can help the user analyze the collected data and use it in an efficient manner.</p>
<p>Lastly, we demonstrate our dataset on an LSTM-based network, where a subset of our data is used to train the network to assign captions to sequences of mouth motions.  Similar 
lipreaders have recently come to prominence, and we demonstrate some of the particular strengths of our dataset with the resuls.</p> 
<h3>2.Related Work</h3>
<p>

(siddharth list stuff here)


</p>
<h3>3.Contributions</h3>
<p>Our contributions with this project are the following :</p>
<ol>
<li>Dataset of 1.5 million samples of face-cropped and isolated mouth images, and related audio samples.</li>
<li>Frame-aligned mapping of the spoken words derived from closed caption files and audio analysis.</li>
<li>Methodology and Codebase to collect and process data from CC-anotated videos.</li>
<li>Self Organizing Maps trained on audio and image data</li>
<li>LSTM-based Lipreader</li>
</ol>
<h3>4.Dataset Details and Methods</h3>
<h4>4.1.Details</h4>
<p>Our dataset differs from other similar datasets that we have found in numerous ways which we hope will serve to illustrate the utility we provide that was not previously available.  
Unlike many datasets, our set is of a single speaker, President Barrack Obama.  While in many ways this could be perceived as a limitation, we hope that the fact that the data was 
collected over a period of 7 years, in multiple environments, serves to mitigate this somewhat.  We also took some steps to help vary the data further, which we expand upon below.  
</p><p>One of the strengths of our dataset is its diversity.  Over 8000 unique words are spoken over the course of our videos, over 3000 of which are spoken 5 or more times (which would yield at least 10 training examples using 
horizontal-axis mirroring of the face images).  These words are spoken in a conversational manner, with sentences of varying cadence, and often indistinct boundaries between words, providing
a more natural setting than a dataset built with the intention of training, for example, an Isolated Word Speech Recognizer.</p>
<p>Another strength of our dataset is in the soure material itself.  The videos we used to collect the data is high definition with clear audio, so the quality of the individual image 
frames and audio samples is very high.  On the other hand, there is a natural variance built into the data since each weekly address source video was recorded one at a time, over the 
span of 7 years.  The lighting and ambient sound, while consistent within a particular video, vary from video to video in a manner that is we have not seen in similar datasets - often the
president is giving his speech in an outdoor location with very different lighting, or in a loud environment like a manufacturing plant, all of which serves to vary the resultant data in a useful manner
and counter the limitation of having only one speaker. </p>
<p>Lastly, the videos have clear and accurate captions provided that, when combined with other processing driven by the audio samples, serve to indicate reasonable estimates of frame 
boundaries for individual words.  These can be verified via synchronization with the audio samples, and also can be fine tuned with speech recognition.</p>
<p>All of the code required to process the source material and construct our dataset is included, except where otherwise specified (i.e. openFace for the face cropping transfomrations).  
We implemented the majority of our scripts in MATLAB, except for the caption-to-frame mapping code which is an Eclipse Java project. </p>
<div style="float: left; padding: 20px">
<img src="images/twoWaysSameSample.jpg" width=210 height=105/>
<p style="font-size: 12px">Figure 2. Two isolated face image samples,the left image using 
</br>the outer eyes and nose as fixed points, the right using the inner
</br>eyes and bottom lip, of the same spoken word from two
</br>different videos.</p>
</div>
<h4>4.2.Image Processing</h4>
<p>To build our dataset we acquired 310 videos of President Obama giving weekly State of the Union addresses, over the period of time from 2009 to 2015, which amounted to over 30 hours of video.  
We sampled these videos to get individual frame images and the assocated audio samples of what the president was saying, using the encoded sampling rate of the video itself, to preserve image alignment and help
with caption alignment.  The videos were originally recorded at either 25 or 30 frames per second.  We then transformed and face-cropped the resultant images using the openFace library.  To accomplish this we chose two sets of different
desired fixed points on the faces (the outside points of the eyes and the nose, and the inside points of the eyes and the lower lip) which were then used to provide a basis to transform the face
image as the speaker's head moved around while speaking.  We used each set of fixed points on half of the image data, in an effort to help expand the diversity of the samples otherwise limited
by the fact that they are generated by the same speaker, since each set of fixed points gives different mouth image results for the same word, as is seen in Figure 2.  Each cropped face image in our dataset is 96 x 96 pixels of RGB data, and we also provide
40 x 40 gray scale images of the isolated mouth.</p>

<h4>4.3.Caption Processing</h4>
<pre><code>%Closed Caption Example
    %This is an example of the .vtt file for a passage of spoken 
    %words from a video, including when these words are to be displayed.
    %(hr:min:sec:ms) --> (hr:min:sec:ms)
    
		00:01:11.400 --> 00:01:13.900
		Now we have to build
		on this progress.
		
		00:01:13.900 --> 00:01:16.667
		Congress should give every
		American the chance to refinance
		
		00:01:16.667 --> 00:01:19.100
		at today's low rates.
		
		00:01:19.100 --> 00:01:20.967
		We should help more qualified
		families get a mortgage
		
		00:01:20.967 --> 00:01:23.166
		and buy their first home.
</code></pre>
<p>Each source video included full transcriptions of the president's speech given in the .vtt closed caption format, which includes the time stamp offset from the beginning of the video
determining when a set of words is to be displayed, as shown above.  Using the assumption that the captions are displayed very close to when the words are spoken, which is a reasonable assumption since 
these videos are not live and are therefore not generating the captions via speech recognition, we developed an algorithm to estimate the frame bounds of each word, to be used to build associations between
sequences of sound samples and words (for speech recognition training) or face/mouth images and words (for lipreader training).</p>
<p>First we had to convert all numeric and symbolic data to words, using the appropriate grammar depending on estimated context and inferred intent.  For example,  
"17.76" would be translated as "seventeen point seven six" while "$17.76" would be translated as "seventeen dollars and seventy six cents"; "1776" would be "seventeen seventy six" and
"$1.776 billion" would be translated as "one point seven seven six billion dollars" and "1,776" would be translated as "one thousand seven hundred and seventy six". This also came into play
with website urls - "www.whitehouse.gov" would be translated as "double u double u double u dot whitehouse dot gov".</p>
<p>We would derive a frame boundary estimate for each word by assigning it a portion of the duration it would be displayed proportional to the word's letter-length footprint in the caption itself, including spaces.
So given a caption displayed for 20 frames, starting at frame 100, consisting of the words "My Fellow Americans", 
we would estimate that "My" would display for 3 frames (100-103), "Fellow" would display for 7 frames (104-110), and "Americans would display for 10 (111-120).</p>
<p>We augmented this estimate by analyzin the audio data.  By taking the RMS power of each frame's sample audio, we searched for values below a certain threshold and used 
them as suggestions of word breaks, as is shown in Figure 3.  We chose the threshold so that we would find more word breaks than there were words spoken in the video to be sure we would find all of them. </p>
<p>Taking these suggestions we would then find the nearest audio-derived word breaks to each audio frame break suggested by the caption data and adjust the caption-derived break accordingly, with the caveat that any
audio suggestion beyond a certain threshold would be ignored.  A clip of an example of our results can be found here : 

https://www.dropbox.com/s/j728vxmkc158h3w/obamaCapSpeechCmp.avi?dl=0

While this method was generally very successful in our random sampling and manual checking of the audio synchronization (with which we covered about 10% of the entire corpus), particularly with larger words,
we realized that with smaller words the small window was often inacccurate by a few frames, as is evidenced in the video linked above, due to cadence variation or just inaccurate assumptions.  When building our training data for the LSTM-based lipreader network, 
this discrepency was mitigated somewhat by the mechanism we used to build the fixed-length sequence windows upon which we trained the network, as is discussed in relation to building our training data in Section 6.2.
</p>
<h3>5.Self Organizing Maps of Audio and Image data</h3>
<p>

(john list stuff here - do we want this?)

</p>

<h3>6.LSTM-based Learning</h3>
<h4>6.1.Long Short-Term Memory</h4>
<p>

(siddharth and john list stuff here)

</p>
<h4>6.2.Constructing Training Data</h4>
<p>For the training data we used in to train the LSTM lipreader, we chose a subset of our word data consisting of 33 words of high frequency of occurence (at least 300 occurences each).  
We isolated the image frames of each occurence in 3-dimensional MATLAB matrices where each column was an image of the cropped grayscale mouth (1600 pixels), each row denoted a particular pixel in this image, and each page denoted a particular
training example.  We built a similar 3-dimensional matrix for our labels, where each word was represented by a 
1-hot vector corresponding to the sequence being classified.  This process was automated, and the code to do this is included, so that
all that is required is a csv file of the desired words fed to the java program that also generates caption frame estimates.  
The resultant file "training_DataNumericDict.csv" is then used by the MATLAB script "testTrainingDataLoad.m" to generate the training data.</p>
<p>Since we used a fixed sequence length across all words, we had to pad or truncate our training data based on the length of a particular word in samples compared to our chosen sequence length, which was 25 frames.  To process 
each word, first we aligned the word's frames to be centered in the sequence map.  Then, if the word was too long, we clipped frames from the edges, favoring ending frames for an odd number of overflow frames.</p>
<p>If the word was shorter than the sequence, we would add extra frames sufficient to cover half of the existing margin, and then fill the rest of the space with empty frames.  So, for example, if a word was 5 frames long, we would add
5 frames of source material both before and after the word, and then add the rest as silence.  This also helped with mis-alignment with very small words, since given the sequence length of 25 
frames (approximately a second) we held great confidence that even our imperfect frame estimates would be sufficient coupled with this buffer of source material (which was larger for the smaller words most likely to be misaligned) 
to find the word in the source material.</p>

<h4>6.3.Models</h4>
<p>

(siddharth list stuff here)

</p>

<h3>7.Results</h3>
<p>

(siddharth list stuff here)

</p>
<h3>8.Future Work</h3>
<p>For future work, there are numerous expansions to our dataset we would like to implement to more accurately present the data we have, as well as to expand our corpus.</p>
<p>One component we would like to implement would be a speech recognition engine to more cleanly specify frame-based word boundaries in the image/audio data.  While our estimation method
worked well, it was not perfect, and a well trained speech recognizer would undoubtedly be able to quickly isolate the appropriate frames for each word in the corpus.  This task is made even easier
given our word suggestions and frame estimations.</p>
<p>Another desireable expansion would be augmenting the dataset with more speakers.  Many news programs and talkshows are available on youtube where an individual speaker is featured, face isolated and professionally recorded.  Using the
collection methods and code we have accumulated it would not be difficult to process these clips in a similar manner and expand the dataset to cover other speakers.</p>


<h3>References</h3>
<p>
//cite SOM

(siddharth list stuff here)


</p>



<pre><code>%Code Example
    %Code Comment
    Code
</code></pre>




<h3>Image Table Example</h3>
<table border=1>
	<tr><td>Example image table 1</td><td>Example  image table 2</td></tr>
	<tr>
		<td><img src="images/LowFreq_ImgNum_1.jpg" width="48%"/></td>
		<td><img src="images/LowFreq_ImgNum_2.jpg" width="48%"/></td>
	</tr>
</table>



<div style="clear:both" >
<p></p>
</div>
</body>
</html>
